# Flexenc

Python library to encode text to tokens (and then to ints, for feeding in into an NLP neural network).
Currently very, very incomplete.


# What is the main idea

in essence, the library provides a set of different tokenizers and token lists. With a decision table, containing some regexps you define which parts of the text you want to send to which tokenizer.

This way you can compose some behaviour, for example different behaviours for numbers as for words. 


# Basic usage example

Here is how you use this library:


```python
from flexenc import iterate_parts, CustomTokenList, CategoryTokenList,ReduceWhitespaceTokenizer, LetterTokenizer,DropTokenizer,NumericTokenizer



decision_table = [
  ("whitespace"     ,r'^(\s+)' ),
  ("numeric"        ,r'^(\d+)' ),                  
  ("word"           ,r'^([a-zA-Z]+)' ),                  
  ("math"           ,r'^([\+\-\*\/=])' ),                  
  ("ignored"        ,r'^([^a-zA-Z\+\-\*\/= \n\t]+)' )                
]

main = CategoryTokenList()
main.addCategory("whitespace",ReduceWhitespaceTokenizer())
main.addCategory("numeric",NumericTokenizer())
main.addCategory("word",LetterTokenizer())
main.addCategory("math",CustomTokenList(["+","-","*","/","="]))
main.addCategory("ignored",DropTokenizer())

def test_just_letters_and_numbers():
  tokens = main.encode(iterate_parts("The quick brown fox jumps over the lazy cow 21 times",decision_table))
  text = main.decode_tokens(tokens)
  assert "The quick brown fox jumps over the lazy cow 21 times" == text

def test_just_a_sum():
  tokens = main.encode(iterate_parts("10 + 7 = 17",decision_table))
  text = main.decode_tokens(tokens)
  assert "10 + 7 = 17" == text

```

  - CategoryTokenList is the main token list containing the different token lists
  - ReduceWhitespaceTokenList can reduce whitespace to a single space. Or to a either a single space or a newline, if you want to conserve line break information
  - NumericTokenizer is a special number processor working for integers from 0 ... 999999.  It will replace 423 with a token list that looks like \[ '4' ,'\*100' ,'2' ,'\*10' ,'3' ,'\*1' ]. The idea is to provide neural networks with more ways to reason about the number than just the positional encoding generated by the network itself. 
  - LetterTokenizer converts letters into tokens.
  - CustomTokenList lets you define tokens and an encode method. If all your tokens are 1 character long and the corresponding regex matches 1 character (as in the example above for the "math" tokens), you don't need to define any encode method yourself.  
  - DropTokenizer emits a single token called "< drop \>". When decoding it will emit a space. TODO: would be nice to emit nothing at all, and have the whitespace tokenizer handle it.



There is also a "learning" WordFragmentTokenList in this repo which allow you to group letters in chunks and having a specifically sized vocabulary for these chunks. This is very similar to how BERT and GPT handle words. But this class isn't finished yet, so for now use LetterTokenizer instead, which should be fine for very simple tasks that don't require  word understanding.



